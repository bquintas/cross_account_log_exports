AWSTemplateFormatVersion: 2010-09-09
Transform: AWS::Serverless-2016-10-31

Description:
  This template deploys a S3 bucket, Kinesis firehose delivery stream to that bucket and
  creates a Cwlogs destination to be used by another account passed in the parameter
  Requires ACCOUNT ID of the Sending Account

Parameters:
  EnvironmentName:
    Description: An environment name that is prefixed to resource names
    Type: String
    Default: KinesisReceiver

  ProviderAccount:
    Description: The ID of the account allowed to send logs
    Type: String

Resources:
  ###Create an Amazon S3 bucket###

  KinesisBucket:
    Type: AWS::S3::Bucket
  ###Create the IAM role that grants Kinesis Data Firehose permission to put data into the bucket###

  FirehosetoS3andLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - firehose.amazonaws.com
            Action:
              - sts:AssumeRole
            Condition:
              StringEquals:
                "sts:ExternalId": !Sub ${AWS::AccountId}
      Path: /
  PermissionsforFirehosetoS3andLambdaRole:
    Type: AWS::IAM::Policy
    Properties:
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Action:
              - s3:AbortMultipartUpload
              - s3:GetBucketLocation
              - s3:GetObject
              - s3:ListBucket
              - s3:ListBucketMultipartUploads
              - s3:PutObject
            Resource:
              - !GetAtt KinesisBucket.Arn
              - !Join ["", [!GetAtt KinesisBucket.Arn, "/*"]]
          - Effect: Allow
            Action:
              - lambda:InvokeFunction
              - lambda:GetFunctionConfiguration
            Resource:
              - !GetAtt DataTransformLambda.Arn
      PolicyName: firehosetos3andlambdapermissions
      Roles:
        - !Ref FirehosetoS3andLambdaRole
  ###create the Kinesis Data Firehose delivery stream###

  kinesisStreamToS3:
    Type: AWS::KinesisFirehose::DeliveryStream
    Properties:
      ExtendedS3DestinationConfiguration:
        BucketARN: !GetAtt KinesisBucket.Arn
        RoleARN: !GetAtt FirehosetoS3andLambdaRole.Arn
        ProcessingConfiguration:
          Enabled: true
          Processors:
            - Parameters:
                - ParameterName: LambdaArn
                  ParameterValue: !GetAtt DataTransformLambda.Arn
              Type: Lambda

  ###create the IAM role that grants CloudWatch Logs the permission to put data into your Kinesis Data Firehose stream###

  CWLtoKinesisRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - logs.eu-west-1.amazonaws.com
            Action:
              - sts:AssumeRole
            Condition:
              StringLike:
                "aws:SourceArn":
                  - !Sub "arn:aws:logs:${AWS::Region}:${ProviderAccount}:*"
                  - !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:*"
      Path: /
  ###Create a permissions policy to define which actions CloudWatch Logs can perform on your account###

  PermissionsforCWL:
    Type: AWS::IAM::Policy
    Properties:
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Action:
              - firehose:*
            Resource:
              - !Sub "arn:aws:firehose:${AWS::Region}:${AWS::AccountId}:*"
      PolicyName: cwltokinesispermissions
      Roles:
        - !Ref CWLtoKinesisRole
  ###create the CloudWatch Logs destination###

  CWLogsDestination:
    Type: AWS::Logs::Destination
    Properties:
      DestinationName: FirehoseDestination
      DestinationPolicy: !Sub |
        {
          "Version": "2012-10-17",
          "Statement": [{
            "Effect": "Allow",
            "Principal": {
              "AWS": "${ProviderAccount}"
            },
            "Action": "logs:PutSubscriptionFilter",
            "Resource": "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:destination:FirehoseDestination"
          }]
        }
      RoleArn: !GetAtt CWLtoKinesisRole.Arn
      TargetArn: !GetAtt kinesisStreamToS3.Arn

  AllowKinesisToCallLambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: "lambda:InvokeFunction"
      FunctionName: !Ref DataTransformLambda
      Principal: firehose.amazonaws.com
      SourceAccount: !Sub ${AWS::AccountId}

  DataTransformLambda:
    Type: AWS::Serverless::Function
    Properties:
      Description: Lambda function to process cw logs
      Runtime: python3.9
      Handler: index.lambda_handler
      MemorySize: 128
      Timeout: 60
      InlineCode: |
        import base64
        import json
        import gzip
        from io import BytesIO
        import boto3

        def transformLogEvent(log_event):
            """Transform each log event.

            The default implementation below just extracts the message and appends a newline to it.

            Args:
            log_event (dict): The original log event. Structure is {"id": str, "timestamp": long, "message": str}

            Returns:
            str: The transformed log event.
            """
            return log_event['message'] + '\n'


        def processRecords(records):
            for r in records:
                data = base64.b64decode(r['data'])
                striodata = BytesIO(data)
                with gzip.GzipFile(fileobj=striodata, mode='r') as f:
                    data = json.loads(f.read())

                recId = r['recordId']
                """
                CONTROL_MESSAGE are sent by CWL to check if the subscription is reachable.
                They do not contain actual data.
                """
                if data['messageType'] == 'CONTROL_MESSAGE':
                    yield {
                        'result': 'Dropped',
                        'recordId': recId
                    }
                elif data['messageType'] == 'DATA_MESSAGE':
                    joinedData = ''.join([transformLogEvent(e) for e in data['logEvents']])
                    dataBytes = joinedData.encode("utf-8")
                    encodedData = base64.b64encode(dataBytes)
                    if len(encodedData) <= 6000000:
                        yield {
                            'data': encodedData,
                            'result': 'Ok',
                            'recordId': recId
                        }
                    else:
                        yield {
                            'result': 'ProcessingFailed',
                            'recordId': recId
                        }
                else:
                    yield {
                        'result': 'ProcessingFailed',
                        'recordId': recId
                    }


        def putRecordsToFirehoseStream(streamName, records, client, attemptsMade, maxAttempts):
            failedRecords = []
            codes = []
            errMsg = ''
            # if put_record_batch throws for whatever reason, response['xx'] will error out, adding a check for a valid
            # response will prevent this
            response = None
            try:
                response = client.put_record_batch(DeliveryStreamName=streamName, Records=records)
            except Exception as e:
                failedRecords = records
                errMsg = str(e)

            # if there are no failedRecords (put_record_batch succeeded), iterate over the response to gather results
            if not failedRecords and response and response['FailedPutCount'] > 0:
                for idx, res in enumerate(response['RequestResponses']):
                    # (if the result does not have a key 'ErrorCode' OR if it does and is empty) => we do not need to re-ingest
                    if 'ErrorCode' not in res or not res['ErrorCode']:
                        continue

                    codes.append(res['ErrorCode'])
                    failedRecords.append(records[idx])

                errMsg = 'Individual error codes: ' + ','.join(codes)

            if len(failedRecords) > 0:
                if attemptsMade + 1 < maxAttempts:
                    print('Some records failed while calling PutRecordBatch to Firehose stream, retrying. %s' % (errMsg))
                    putRecordsToFirehoseStream(streamName, failedRecords, client, attemptsMade + 1, maxAttempts)
                else:
                    raise RuntimeError('Could not put records after %s attempts. %s' % (str(maxAttempts), errMsg))


        def putRecordsToKinesisStream(streamName, records, client, attemptsMade, maxAttempts):
            failedRecords = []
            codes = []
            errMsg = ''
            # if put_records throws for whatever reason, response['xx'] will error out, adding a check for a valid
            # response will prevent this
            response = None
            try:
                response = client.put_records(StreamName=streamName, Records=records)
            except Exception as e:
                failedRecords = records
                errMsg = str(e)

            # if there are no failedRecords (put_record_batch succeeded), iterate over the response to gather results
            if not failedRecords and response and response['FailedRecordCount'] > 0:
                for idx, res in enumerate(response['Records']):
                    # (if the result does not have a key 'ErrorCode' OR if it does and is empty) => we do not need to re-ingest
                    if 'ErrorCode' not in res or not res['ErrorCode']:
                        continue

                    codes.append(res['ErrorCode'])
                    failedRecords.append(records[idx])

                errMsg = 'Individual error codes: ' + ','.join(codes)

            if len(failedRecords) > 0:
                if attemptsMade + 1 < maxAttempts:
                    print('Some records failed while calling PutRecords to Kinesis stream, retrying. %s' % (errMsg))
                    putRecordsToKinesisStream(streamName, failedRecords, client, attemptsMade + 1, maxAttempts)
                else:
                    raise RuntimeError('Could not put records after %s attempts. %s' % (str(maxAttempts), errMsg))


        def createReingestionRecord(isSas, originalRecord):
            if isSas:
                return {'data': base64.b64decode(originalRecord['data']), 'partitionKey': originalRecord['kinesisRecordMetadata']['partitionKey']}
            else:
                return {'data': base64.b64decode(originalRecord['data'])}


        def getReingestionRecord(isSas, reIngestionRecord):
            if isSas:
                return {'Data': reIngestionRecord['data'], 'PartitionKey': reIngestionRecord['partitionKey']}
            else:
                return {'Data': reIngestionRecord['data']}


        def lambda_handler(event, context):
            isSas = 'sourceKinesisStreamArn' in event
            streamARN = event['sourceKinesisStreamArn'] if isSas else event['deliveryStreamArn']
            region = streamARN.split(':')[3]
            streamName = streamARN.split('/')[1]
            records = list(processRecords(event['records']))
            projectedSize = 0
            dataByRecordId = {rec['recordId']: createReingestionRecord(isSas, rec) for rec in event['records']}
            putRecordBatches = []
            recordsToReingest = []
            totalRecordsToBeReingested = 0

            for idx, rec in enumerate(records):
                if rec['result'] != 'Ok':
                    continue
                projectedSize += len(rec['data']) + len(rec['recordId'])
                # 6000000 instead of 6291456 to leave ample headroom for the stuff we didn't account for
                if projectedSize > 6000000:
                    totalRecordsToBeReingested += 1
                    recordsToReingest.append(
                        getReingestionRecord(isSas, dataByRecordId[rec['recordId']])
                    )
                    records[idx]['result'] = 'Dropped'
                    del(records[idx]['data'])

                # split out the record batches into multiple groups, 500 records at max per group
                if len(recordsToReingest) == 500:
                    putRecordBatches.append(recordsToReingest)
                    recordsToReingest = []

            if len(recordsToReingest) > 0:
                # add the last batch
                putRecordBatches.append(recordsToReingest)

            # iterate and call putRecordBatch for each group
            recordsReingestedSoFar = 0
            if len(putRecordBatches) > 0:
                client = boto3.client('kinesis', region_name=region) if isSas else boto3.client('firehose', region_name=region)
                for recordBatch in putRecordBatches:
                    if isSas:
                        putRecordsToKinesisStream(streamName, recordBatch, client, attemptsMade=0, maxAttempts=20)
                    else:
                        putRecordsToFirehoseStream(streamName, recordBatch, client, attemptsMade=0, maxAttempts=20)
                    recordsReingestedSoFar += len(recordBatch)
                    print('Reingested %d/%d records out of %d' % (recordsReingestedSoFar, totalRecordsToBeReingested, len(event['records'])))
            else:
                print('No records to be reingested')

            return {"records": records}

Outputs:
  CWLogsDestination:
    Description: Arn of the CWLogs destination
    Value: !GetAtt CWLogsDestination.Arn
